{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 18th, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height(cm)  weight(kg) t-shirt size\n",
      "0         158          58            M\n",
      "1         163          61            M\n",
      "2         165          61            L\n",
      "3         168          66            L\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df[\"height(cm)\"] = [158,163,165,168]\n",
    "df[\"weight(kg)\"] = [58,61,61,66]\n",
    "df[\"t-shirt size\"] = [\"M\",\"M\",\"L\",\"L\"]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML (Machine Learning)\n",
    "* Supervised learning: labeled data (e.g. there is an attribute (AKA feature)that you are interested in predicting for unseen instances)\n",
    "    * The attribute is often called the \"class\" or the \"class label\"\n",
    "    * The attribute is categorical... classification\n",
    "    * The attribute is numeric... regression\n",
    "    * Example algorithm we are using today kNN (k nearest neighbors)\n",
    "* Unsupervised learning: unlabeled data\n",
    "    * Example algorithm: k-means clustering\n",
    "             \n",
    "\n",
    "## Supervised Learning \n",
    "* Need a way to divide a dataset into a training set and a testing set\n",
    "    * The training set is used to build/train a algorithm/model\n",
    "    * The testing set is used to evaluate the algorithm/model\n",
    "    * The training test and the testing set *are different*\n",
    "* Example\n",
    "    * We have this super tiny t-shirt sizes dataset\n",
    "        * 4 instances\n",
    "        * 3 attributes (1 is the class)\n",
    "        * Goal is to use the height and weight attributes to predict the t-shirt size\n",
    "        * We will do this for a test set with a single unseen instance\n",
    "            * height=161 weight=63 t-shirt=?\n",
    "            * Let's say the \"ground truth value\" is M (Medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Algorithm\n",
    "* Identify the k nearest neighbors in the training set to a test set instance\n",
    "    * The most frequently occuring class label amongst the k nearest neighbors will be the class label prediction for the unseen instance\n",
    "* We need a way to measure \"nearness\" AKA \"closseness\"\n",
    "    * 2D: Pythagorean theorem\n",
    "    * ND: Euclidean distance formula: $dist(a,b) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i) ^2}$\n",
    "* We need to normalize (AKA scale) our attributes so we don't have an unanticipated weighting of one attribute more than another (e.g. height has a larger scale then weight, so it will dominate the formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height(cm)  weight(kg)\n",
      "0         158          58\n",
      "1         163          61\n",
      "2         165          61\n",
      "3         168          66\n",
      "0    M\n",
      "1    M\n",
      "2    L\n",
      "3    L\n",
      "Name: t-shirt size, dtype: object\n",
      "[[161, 63]]\n"
     ]
    }
   ],
   "source": [
    "X_train = df.drop(\"t-shirt size\",axis=1)\n",
    "print(X_train)\n",
    "y_train = df[\"t-shirt size\"]\n",
    "print(y_train)\n",
    "X_test = [[161,63]]\n",
    "print(X_test)\n",
    "\n",
    "# Step 1: Normalize the x data\n",
    "# Step 2: Compute the distances to each unseen instance in the test set\n",
    "# Step 3: Apply majority voting to the k=(3) closest distance labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic k-NN Algorithm\n",
    "```\n",
    "Input: list of rows, no of atts (n where nth is label), instance to classify, k\n",
    "def kNN_classifier(training_set, n, instance, k):\n",
    "    row_distances = []\n",
    "    for row in training_set:\n",
    "        d = distance(row, instance, n - 1)\n",
    "        row_distances.append([d, row])\n",
    "    top_k_rows = get_top_k(row_distances, k)\n",
    "    label = select_class_label(top_k_rows)\n",
    "    return label\n",
    "```\n",
    "\n",
    "## kNN Example\n",
    "Example adapted from [this kNN example](https://people.revoledu.com/kardi/tutorial/KNN/KNN_Numerical-example.html)\n",
    "\n",
    "Suppose we have the following dataset that has two attributes (acid durability and strength) and a class attribute (whether a special paper tissue is good or not):\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|7|7|Bad|\n",
    "|7|4|Bad|\n",
    "|3|4|Good|\n",
    "|1|4| Good|\n",
    "\n",
    "Now the factory produces a new paper tissue with acid durability = 3 seconds and strength = 7 kg/square meter. Can we predict what the classification of this new tissue is? Use kNN with $k$ = 3. \n",
    "\n",
    "### Make a Prediction Manually\n",
    "Steps:\n",
    "1. Normalize\n",
    "1. Compute distance of each training instance to the test instance\n",
    "1. Determine the majority classification of the $k$ closest instances... this is your prediction for the test instance\n",
    "\n",
    "After normalization:\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|1|1|Bad|\n",
    "|1|0|Bad|\n",
    "|0.33|0|Good|\n",
    "|0|0| Good|\n",
    "\n",
    "Test instance normalization: 0.33, 1\n",
    "\n",
    "Distances:\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|Distance|\n",
    "|-|-|-|-|\n",
    "|1|1|Bad|0.66|\n",
    "|1|0|Bad|1.203|\n",
    "|0.33|0|Good|1.0|\n",
    "|0|0| Good|1.05|\n",
    "\n",
    "Work:\n",
    "* $\\sqrt{(1-0.33)^2 + (1-1)^2} = 0.66$\n",
    "* $\\sqrt{(1-0.33)^2 + (0-1)^2} = 1.203$\n",
    "* $\\sqrt{(0.33-0.33)^2 + (0-1)^2} = 1.0$\n",
    "* $\\sqrt{(0-0.33)^2 + (0-1)^2} = 1.05$\n",
    "\n",
    "Majority classification: \n",
    "1 Bad (0.66) and 2 Goods (1.0 an 1.05) => Good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction with Scikit-Learn\n",
    "Steps:\n",
    "1. Load data\n",
    "1. Normalize\n",
    "1. Train kNN classifier with training set\n",
    "1. Test kNN classifier on test instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train) # often combined into one step, using fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Acid durability (seconds)  Strength (kg/square meter) Classification\n",
      "0                          7                           7            Bad\n",
      "1                          7                           4            Bad\n",
      "2                          3                           4           Good\n",
      "3                          1                           4           Good\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "\n",
    "data = [[7, 7, \"Bad\"], [7, 4, \"Bad\"], [3, 4, \"Good\"], [1, 4, \"Good\"]]\n",
    "df = pd.DataFrame(data, columns=[\"Acid durability (seconds)\", \"Strength (kg/square meter)\", \"Classification\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 4.]\n",
      "[7. 7.]\n",
      "[[1.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.33333333 0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train = df.drop(\"Classification\", axis=1)\n",
    "y_train = df[\"Classification\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "print(scaler.data_min_)\n",
    "print(scaler.data_max_)\n",
    "\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "print(X_train_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use minmax scaling because it corrects outliers, making the formulas work \n",
    "\n",
    "normalizes attributes to the same scale so one isnt inheritaly weighted more than the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benpu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:446: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train_normalized, y_train)\n",
    "\n",
    "# test\n",
    "X_test = pd.Series([3, 7], index=df.columns.drop(\"Classification\"))\n",
    "X_test = scaler.transform([X_test])\n",
    "y_test_prediction = neigh.predict(X_test)\n",
    "print(y_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation\n",
    "* In our previous demo, we had 1 instance in our \"test set\"\n",
    "    * If our classifier predicted this instance's class correctly, accuracy = 100%\n",
    "    * If our classifier predicted this instance's class incorrectly, accuracy = 0%\n",
    "    * This is very strict. We only gave it one chance\n",
    "* Notes\n",
    "    * We should use a \"large\" test set to get a better picture of how our classifier is performing\n",
    "    * Accuracy doesn't tell the whole story...\n",
    "        * E.g. 100 samples... 99 M, 1 L\n",
    "        * And our classifier simply only predicts M\n",
    "        * We have 99% accuracy, yet this isnt really good\n",
    "        * Accuracy only makes sense when your class labels are near evenly distributed\n",
    "* Given a dataset, we need a way to \"divide\" our dataset into a training set and a test set\n",
    "    * A few ways to do this...\n",
    "        1. Hold out method \n",
    "        1. Random subsampling\n",
    "        1. Cross validation\n",
    "        1. Boostrap method\n",
    "\n",
    "## Hold out Method\n",
    "* \"hold out\" a certain number or percatage of instances in a dataset for testing\n",
    "    * Train on the remaining instances\n",
    "    * typically choose a standard split or percentage\n",
    "        * E.g. 2:1 split: 1/3 of data held out for testing; 2/3 used for training\n",
    "        * E.g. 25% of data held out for testing; 75% used for training\n",
    "            * Default for sklearn's `train_test_slit()`\n",
    "        \n",
    "Stratify: Lets be more intentional on producing the test set  \n",
    "equal distribution in the test set that matches our dataset.  \n",
    "If the dataset is basically 50/50, make sure the test set is basically 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dtype=float)\n",
    "\n",
    "df = pd.read_csv(\"shirt_sizes_long.csv\")\n",
    "\n",
    "X = df.drop(\"t-shirt size\",axis=1)\n",
    "y = df[\"t-shirt size\"]\n",
    "\n",
    "scaler= MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# print(X)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n",
      "12    L\n",
      "6     M\n",
      "7     L\n",
      "0     M\n",
      "10    L\n",
      "Name: t-shirt size, dtype: object\n",
      "['L' 'M' 'M' 'M' 'L']\n",
      "['L', 'M', 'L', 'M', 'L']\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "print(len(X) * 0.25)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0,stratify=y)\n",
    "#print(X_train) \n",
    "#print(y_train)\n",
    "#print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, metric=\"euclidean\")\n",
    "knn_clf.fit(X_train,y_train)\n",
    "y_predicted = knn_clf.predict(X_test)\n",
    "print(y_predicted)\n",
    "print(list(y_test)) # 80%\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L' 'M' 'L' 'M' 'L']\n",
      "['L', 'M', 'L', 'M', 'L']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Again but with a decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_predicted = tree_clf.predict(X_test)\n",
    "print(y_predicted)\n",
    "print(list(y_test)) # 100%\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subsampling \n",
    "* Perform the hold out method k times (diff k from kNN)\n",
    "* Accuracy is the mean accuracy over the k runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "* With random subsampling, we are not guarenteed that each instance ends up in a test set at least once\n",
    "* With cross validation, we are more intentional about our \"partitions\"\n",
    "* Algorithm: Divide the dataset into k folds (also a diff k)\n",
    "    * For each fold:\n",
    "        * Hold out the fold and test on it\n",
    "        * Train on the remaining fold\n",
    "* With this algorithm each instance is tested exactly 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n",
      "[0.75       0.5        1.         1.         0.66666667]\n",
      "['M' 'M' 'M' 'M' 'M' 'M' 'L' 'M' 'L' 'M' 'M' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      "0.7777777777777778\n",
      "<class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "[0.5        0.5        1.         1.         0.66666667]\n",
      "['M' 'M' 'L' 'M' 'M' 'M' 'L' 'M' 'M' 'M' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      "0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict \n",
    "\n",
    "# run 5-fold cross validation for both the knn and tree\n",
    "for clf in (knn_clf, tree_clf):\n",
    "    print(type(clf))\n",
    "    # a lazy approach\n",
    "    accuracies = cross_val_score(clf, X, y, cv=5) # cv is the amount of folds\n",
    "    print(accuracies)\n",
    "    # a better approach\n",
    "    y_predicted = cross_val_predict(clf, X, y, cv=5)\n",
    "    print(y_predicted)\n",
    "    accuracy = accuracy_score(y, y_predicted)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of cross validation\n",
    "* Stratified k fold cross validation: roughly the same distribution of class labels in each fold\n",
    "* LOOCV (leave one out cross validation): k = N: Each fold contains exactly one instance\n",
    "    * Good for when you need as much training data as possible\n",
    "    * inefficient"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42f2e04143d7207fe24b7231a7967b30a58f6a4af2031ceb790690a1f6850973"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
